import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import graphviz
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import KFold
import warnings

# --- 1. Machine Learning & Causal Imports ---
try:
    from xgboost import XGBRegressor
except ImportError:
    st.error("üö® Please install xgboost: `pip install xgboost`")
    st.stop()

try:
    from econml.dml import LinearDML
except ImportError:
    st.error("üö® Critical Missing Library: Please run `pip install econml`")
    st.stop()

try:
    from causalml.inference.meta import BaseSRegressor, BaseTRegressor, BaseXRegressor
except ImportError:
    st.error("üö® Critical Missing Library: Please run `pip install causalml`")
    st.stop()

from fpdf import FPDF

# Suppress warnings
warnings.filterwarnings('ignore')

# ==========================================
# 2. Page Configuration & Professional CSS
# ==========================================
st.set_page_config(layout="wide", page_title="Universal Causal Dashboard", page_icon="üîÆ")

st.markdown("""
<style>
    /* 1. Load Font */
    @import url('https://fonts.googleapis.com/css2?family=Josefin+Sans&display=swap');
    
    /* 2. Global Font */
    html, body, [class*="css"], font, div, span, p, text {
        font-family: 'Josefin Sans', sans-serif !important;
    }
    
    /* 3. Headers */
    h1, h2, h3, h4, h5, h6 {
        color: #0f172a !important;
        font-family: 'Josefin Sans', sans-serif !important;
        font-weight: 800 !important;
        letter-spacing: -0.5px;
    }
    
    /* 4. Metrics & Tabs */
    [data-testid="stMetricValue"] { color: #000000 !important; font-family: 'Josefin Sans', sans-serif !important; }
    .stTabs [data-baseweb="tab"] { font-family: 'Josefin Sans', sans-serif !important; font-size: 1.2rem; font-weight: 600; }
    
    /* 5. Custom Containers */
    .stMetric {
        background-color: #ffffff;
        padding: 15px 20px;
        border-radius: 12px;
        border: 1px solid #e2e8f0;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
    }
    
    /* 6. Theory Box */
    .theory-box {
        background-color: #f0f9ff;
        border-left: 5px solid #0ea5e9;
        padding: 15px;
        margin-bottom: 20px;
        border-radius: 4px;
        font-size: 0.95rem;
        color: #334155;
    }
    
    .evaluation-box {
        background-color: #fdf2f8;
        border-left: 5px solid #db2777;
        padding: 15px;
        margin-bottom: 20px;
        border-radius: 4px;
        font-size: 0.95rem;
        color: #334155;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 3. Data Processing & Logic
# ==========================================
@st.cache_data
def load_data(file):
    return pd.read_csv(file)

def auto_feature_eng(df, target, treatment):
    df = df.copy()
    lag_cols = []
    # Create 3 days of lag history (Context)
    for i in [1, 2, 3]:
        col_name = f'{target}_Lag{i}'
        df[col_name] = df[target].shift(i)
        lag_cols.append(col_name)
    
    df = df.dropna().reset_index(drop=True)
    
    # PCA for Latent Market State (Momentum)
    scaler = StandardScaler()
    if lag_cols:
        pca = PCA(n_components=1)
        df['Latent_Market_State'] = pca.fit_transform(scaler.fit_transform(df[lag_cols]))
    else:
        df['Latent_Market_State'] = 0
        
    return df

# ==========================================
# 4. Engine 1: EconML (Double Machine Learning)
# ==========================================
class RealCausalEngine:
    def __init__(self):
        # 3-Fold Cross-Fitting for Robustness
        self.dml_est = LinearDML(
            model_y=RandomForestRegressor(n_estimators=50, min_samples_leaf=5),
            model_t=RandomForestRegressor(n_estimators=50, min_samples_leaf=5),
            random_state=42,
            cv=3
        )
        self.base_model = XGBRegressor(n_estimators=100, random_state=42)
        self.features = []
        self.treatment = ""
        self.confounders = []

    def train(self, df, target_col, treatment_col, confounders, heterogeneity_cols=None):
        self.target = target_col
        self.treatment = treatment_col
        self.confounders = confounders
        X = df[heterogeneity_cols] if heterogeneity_cols else None
        W = df[confounders]
        Y = df[target_col]
        T = df[treatment_col]

        with st.spinner("üß† Engines warming up... DML running 3-Fold Cross-Fitting..."):
            self.dml_est.fit(Y, T, X=X, W=W)
            all_feats = [treatment_col] + confounders + (heterogeneity_cols if heterogeneity_cols else [])
            self.base_model.fit(df[all_feats], Y)
            self.features = all_feats

    def get_causal_effect(self, X_pred):
        return self.dml_est.effect(X_pred)

    def predict_counterfactual(self, df_input, new_price_col):
        base_pred = self.base_model.predict(df_input[self.features])
        delta_t = df_input[new_price_col] - df_input[self.treatment]
        
        if 'Latent_Market_State' in df_input.columns:
            theta = self.dml_est.effect(df_input[['Latent_Market_State']])
        else:
            theta = self.dml_est.const_marginal_effect(df_input[self.confounders])
            
        counterfactual_sales = base_pred + (theta * delta_t)
        return np.maximum(counterfactual_sales, 0)

# ==========================================
# 5. Engine 2: CausalML (Meta-Learners)
# ==========================================
def train_meta_learners(df, target_col, treatment_col, feature_cols):
    X = df[feature_cols]
    y = df[target_col]
    w = df[treatment_col].copy()
    
    # Auto-Binarize Treatment for Meta-Learners (High vs Low)
    if w.nunique() > 2:
        median_val = w.median()
        w_binary = (w > median_val).astype(int)
    else:
        w_binary = w.astype(int)

    results = {}
    
    # 1. S-Learner (Base Line)
    learner_s = BaseSRegressor(learner=LinearRegression())
    cate_s = learner_s.fit_predict(X=X, treatment=w_binary, y=y)
    results['S-Learner'] = cate_s.flatten()

    # 2. T-Learner (Separation)
    learner_t = BaseTRegressor(learner=XGBRegressor(n_estimators=50, verbosity=0))
    cate_t = learner_t.fit_predict(X=X, treatment=w_binary, y=y)
    results['T-Learner'] = cate_t.flatten()
    
    # 3. X-Learner (Advanced)
    learner_x = BaseXRegressor(learner=XGBRegressor(n_estimators=50, verbosity=0))
    cate_x = learner_x.fit_predict(X=X, treatment=w_binary, y=y)
    results['X-Learner'] = cate_x.flatten()
        
    return pd.DataFrame(results)

# ==========================================
# 6. Main Application Layout
# ==========================================
col_title, col_logo = st.columns([5, 1])
with col_title:
    st.title("üß† Causal AI Strategy Dashboard")
    st.markdown("Quantify the **True Impact** of your decisions using Double Machine Learning.")

# --- Sidebar ---
with st.sidebar:
    st.header("üéõÔ∏è Control Tower")
    st.info("Upload your historical sales data to begin causal inference.")
    
    uploaded_file = st.file_uploader("Upload CSV Data", type="csv")
    
    if uploaded_file:
        raw_df = load_data(uploaded_file)
        cols = raw_df.select_dtypes(include=np.number).columns.tolist()
        
        st.markdown("### 1. Model Configuration")
        target_col = st.selectbox("üéØ Target (Outcome Y)", cols, index=0)
        treatment_col = st.selectbox("üíä Treatment (Input T)", cols, index=1)
        
        avail_cols = [c for c in cols if c not in [target_col, treatment_col]]
        confounders = st.multiselect("üå™Ô∏è Confounders (Controls W)", avail_cols, default=avail_cols[:2])
        
        st.markdown("### 2. Execution")
        if st.button("üöÄ Run Causal Engine", type="primary", use_container_width=True):
            st.session_state['run'] = True
            st.session_state['cate_results'] = None
            st.session_state['fold_metrics'] = None
            st.session_state['ols_fold_metrics'] = None
    else:
        st.caption("Waiting for data...")

# --- Main Content ---
if st.session_state.get('run', False) and uploaded_file:
    
    # 1. Feature Engineering
    df_eng = auto_feature_eng(raw_df, target_col, treatment_col)
    train_size = int(len(df_eng) * 0.8)
    train_df = df_eng.iloc[:train_size]
    test_df = df_eng.iloc[train_size:].reset_index(drop=True)
    all_confounders = confounders + [c for c in df_eng.columns if 'Lag' in c]

    # --- Heads-Up Display (Metrics Row) ---
    st.markdown("---")
    m_col1, m_col2, m_col3, m_col4 = st.columns(4)
    m_col1.metric("Observation Window", f"{len(df_eng)} Periods")
    m_col2.metric("Target Variable", target_col)
    m_col3.metric("Treatment Variable", treatment_col)
    m_col4.metric("Confounders Tracked", len(all_confounders))
    st.markdown("---")

    # --- Tabs ---
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "‚ö° Insights & Elasticity", 
        "üîÆ Sensitivity Simulator", 
        "‚öîÔ∏è Model Battle", 
        "‚öñÔ∏è Evaluation",
        "üìã Executive Report"
    ])
    
    # ==========================
    # TAB 1: DML Insights
    # ==========================
    with tab1:
        st.subheader("Separating Signal from Noise")
        
        c_dag, c_explain = st.columns([1, 2])
        with c_dag:
            dot = graphviz.Digraph()
            dot.attr(rankdir='LR', size='8,5')
            dot.attr('node', shape='box', style='filled,rounded', fontname='Inter')
            dot.node('T', 'Treatment', fillcolor='#d1fae5', color='#059669') 
            dot.node('Y', 'Outcome', fillcolor='#dbeafe', color='#2563eb')
            dot.node('W', 'Confounders', shape='ellipse', fillcolor='#fee2e2', color='#dc2626')
            
            dot.edge('T', 'Y', label=' Causal Link', color='#059669', penwidth='2.0')
            dot.edge('W', 'T', style='dashed', color='#94a3b8')
            dot.edge('W', 'Y', style='dashed', color='#94a3b8')
            st.graphviz_chart(dot)
            
        with c_explain:
            st.markdown("""
            <div class='theory-card'>
            <b>The "Noise Cancellation" Logic:</b><br>
            Standard correlations are biased because <b>Confounders</b> (Red) affect both your decision (T) and the outcome (Y).
            <br><br>
            We use <b>Double Machine Learning</b> to "block" the red dashed lines, isolating the pure green causal link.
            </div>
            """, unsafe_allow_html=True)

        # Run Engine
        engine = RealCausalEngine()
        engine.train(train_df, target_col, treatment_col, all_confounders, heterogeneity_cols=['Latent_Market_State'])
        
        effects = engine.get_causal_effect(test_df[['Latent_Market_State']])
        avg_elasticity = np.mean(effects)
        naive_corr = test_df[[treatment_col, target_col]].corr().iloc[0,1]
        
        # Key Metrics
        k1, k2, k3 = st.columns(3)
        k1.metric("True Causal Elasticity", f"{avg_elasticity:.3f}", 
                  help="The actual impact of Treatment on Target, free of bias.")
        
        bias_delta = avg_elasticity - naive_corr
        k2.metric("Naive Correlation", f"{naive_corr:.3f}", 
                  delta=f"Bias Detected: {bias_delta:.3f}", delta_color="inverse",
                  help="The raw correlation found in Excel. Often misleading.")
        
        bias_status = "Significant Bias" if abs(bias_delta) > 0.1 else "Clean Data"
        k3.metric("Data Reliability", bias_status, 
                  delta="Corrected via DML" if abs(bias_delta) > 0.1 else "Verified",
                  help="If Bias is Significant, traditional models will fail.")

        # Heterogeneity Chart
        st.markdown("#### üìâ Dynamic Sensitivity Analysis")
        st.caption("How does the causal effect change based on market momentum?")
        
        viz_df = pd.DataFrame({
            'Market Momentum': test_df['Latent_Market_State'],
            'Impact': effects
        })
        
        # Trend Curve
        z = np.polyfit(viz_df['Market Momentum'], viz_df['Impact'], 3)
        p = np.poly1d(z)
        x_trend = np.linspace(viz_df['Market Momentum'].min(), viz_df['Market Momentum'].max(), 100)
        y_trend = p(x_trend)

        fig_hte = px.scatter(viz_df, x='Market Momentum', y='Impact', 
                             color='Impact', color_continuous_scale='Tealgrn', opacity=1.0)
        
        fig_hte.add_trace(go.Scatter(x=x_trend, y=y_trend, mode='lines', 
                                     line=dict(color='rgba(239, 68, 68, 0.6)', width=4), 
                                     name='Trend'))
        
        fig_hte.update_layout(template="plotly_white", xaxis_title="Market Momentum (PCA)", yaxis_title="Causal Impact")
        fig_hte.add_hline(y=0, line_dash="dot", line_color="gray")
        st.plotly_chart(fig_hte, use_container_width=True)

    # ==========================
    # TAB 2: Sensitivity Simulator
    # ==========================
    with tab2:
        st.subheader("üîÆ Multi-Scenario Simulator")
        st.markdown("Compare your **Proposed Strategy** against Higher and Lower price alternatives.")
        
        col_in, col_out = st.columns([1, 2])
        with col_in:
            st.markdown("### üõ†Ô∏è Adjust Strategy")
            curr_avg = float(test_df[treatment_col].mean())
            
            # 1. Main Price Slider
            price_main = st.slider("Proposed Treatment Value (Center)", 
                                  min_value=float(test_df[treatment_col].min()), 
                                  max_value=float(test_df[treatment_col].max()), 
                                  value=curr_avg)
            
            # 2. Comparison Mode Selector
            comp_mode = st.radio("Comparison Mode", ["Percentage (+/- %)", "Manual Prices ($)"], horizontal=True)
            
            if "Percentage" in comp_mode:
                sensitivity = st.slider("Comparison Interval (+/- %)", min_value=1, max_value=20, value=5)
                price_low = price_main * (1 - sensitivity/100)
                price_high = price_main * (1 + sensitivity/100)
                scenario_labels = [f"Lower (-{sensitivity}%)", "Proposed", f"Higher (+{sensitivity}%)"]
            else:
                c1, c2 = st.columns(2)
                price_low = c1.number_input("Lower Price Scenario ($)", value=float(price_main*0.95))
                price_high = c2.number_input("Higher Price Scenario ($)", value=float(price_main*1.05))
                scenario_labels = ["Scenario A (Low)", "Proposed", "Scenario B (High)"]
            
            st.markdown("### üì¶ Inventory Specs")
            lead_time = st.number_input("Lead Time (Days)", value=5)
            
            st.markdown("---")
            st.caption(f"Current Elasticity: **{avg_elasticity:.3f}**")

        with col_out:
            # Run Predictions for all 3 Scenarios
            # Main
            sim_df = test_df.copy()
            sim_df[f'New_{treatment_col}'] = price_main
            cf_main = engine.predict_counterfactual(sim_df, f'New_{treatment_col}')
            
            # High
            sim_df_high = test_df.copy()
            sim_df_high[f'New_{treatment_col}'] = price_high
            cf_high = engine.predict_counterfactual(sim_df_high, f'New_{treatment_col}')
            
            # Low
            sim_df_low = test_df.copy()
            sim_df_low[f'New_{treatment_col}'] = price_low
            cf_low = engine.predict_counterfactual(sim_df_low, f'New_{treatment_col}')
            
            # Metrics Calculation
            total_act = test_df[target_col].sum()
            
            # Main
            total_sim = cf_main.sum()
            rev_sim = total_sim * price_main
            std_main = np.std(cf_main)
            opt_stock_main = (total_sim/len(sim_df) * lead_time) + (std_main * 1.645 * np.sqrt(lead_time))

            # Low
            total_sim_low = cf_low.sum()
            rev_sim_low = total_sim_low * price_low
            std_low = np.std(cf_low)
            opt_stock_low = (total_sim_low/len(sim_df) * lead_time) + (std_low * 1.645 * np.sqrt(lead_time))

            # High
            total_sim_high = cf_high.sum()
            rev_sim_high = total_sim_high * price_high
            std_high = np.std(cf_high)
            opt_stock_high = (total_sim_high/len(sim_df) * lead_time) + (std_high * 1.645 * np.sqrt(lead_time))

            # Display Main Metrics (Top of Tab)
            s1, s2, s3 = st.columns(3)
            s1.metric("Projected Demand (Center)", f"{total_sim:,.0f}", delta=f"{(total_sim-total_act):,.0f}")
            s2.metric("Projected Value (Center)", f"${rev_sim:,.0f}", delta=f"${(rev_sim - (total_act*curr_avg)):,.0f}")
            s3.metric("Optimal Safety Stock", f"{opt_stock_main:,.0f}", help="Based on Center Scenario")

            # Plotting
            fig_cf = go.Figure()
            fig_cf.add_trace(go.Scatter(y=test_df[target_col], name="Historical Actuals", line=dict(color='#cbd5e1', width=2)))
            fig_cf.add_trace(go.Scatter(y=cf_low, name=f"{scenario_labels[0]} (${price_low:.2f})", line=dict(color='#10b981', width=2, dash='dash')))
            fig_cf.add_trace(go.Scatter(y=cf_main, name=f"{scenario_labels[1]} (${price_main:.2f})", line=dict(color='#0ea5e9', width=4)))
            fig_cf.add_trace(go.Scatter(y=cf_high, name=f"{scenario_labels[2]} (${price_high:.2f})", line=dict(color='#ef4444', width=2, dash='dash')))

            fig_cf.update_layout(title="Scenario Comparison", template="plotly_white", hovermode="x unified", legend=dict(orientation="h", y=1.1))
            st.plotly_chart(fig_cf, use_container_width=True)
            
            # Comparison Table
            st.markdown("#### üìä Scenario Breakdown")
            comp_data = {
                "Scenario": scenario_labels,
                "Price Point": [f"${price_low:.2f}", f"${price_main:.2f}", f"${price_high:.2f}"],
                "Total Demand": [f"{total_sim_low:,.0f}", f"{total_sim:,.0f}", f"{total_sim_high:,.0f}"],
                "Total Revenue": [f"${rev_sim_low:,.0f}", f"${rev_sim:,.0f}", f"${rev_sim_high:,.0f}"],
                "Rec. Safety Stock": [f"{opt_stock_low:,.0f}", f"{opt_stock_main:,.0f}", f"{opt_stock_high:,.0f}"]
            }
            st.dataframe(pd.DataFrame(comp_data), use_container_width=True)

    # ==========================
    # TAB 3: Model Battle
    # ==========================
    with tab3:
        st.subheader("‚öîÔ∏è Battle of the Meta-Learners")
        st.markdown("Compare specialized Causal architectures to validate the finding.")
        
        if st.session_state.get('cate_results') is None:
             if st.button("üèÅ Start Tournament", use_container_width=True):
                 meta_feats = all_confounders + ['Latent_Market_State']
                 with st.spinner("Running Causal Tournament..."):
                    cate_results = train_meta_learners(df_eng, target_col, treatment_col, meta_feats)
                    st.session_state['cate_results'] = cate_results
        
        if st.session_state.get('cate_results') is not None:
            cate_results = st.session_state['cate_results']
            
            st.markdown("#### 1. Impact Distribution (Uplift)")
            st.caption("Wide distribution = High Personalization Potential. Narrow = Universal Effect.")
            
            fig_hist = go.Figure()
            colors = ['#94a3b8', '#2dd4bf', '#3b82f6'] 
            for i, model in enumerate(cate_results.columns):
                fig_hist.add_trace(go.Histogram(x=cate_results[model], name=model, opacity=0.7, marker_color=colors[i]))
            
            fig_hist.update_layout(barmode='overlay', template="plotly_white", xaxis_title="Estimated Causal Effect")
            st.plotly_chart(fig_hist, use_container_width=True)
            
            st.markdown("#### 2. Agreement Matrix")
            fig_corr = px.imshow(cate_results.corr(), text_auto=True, color_continuous_scale='Blues')
            st.plotly_chart(fig_corr, use_container_width=True)
            
            s_mean = cate_results['S-Learner'].mean()
            t_mean = cate_results['T-Learner'].mean()
            x_mean = cate_results['X-Learner'].mean()
            
            st.markdown("### üèõÔ∏è Tournament Consensus")
            if (s_mean > 0 and t_mean > 0 and x_mean > 0) or (s_mean < 0 and t_mean < 0 and x_mean < 0):
                st.success(f"‚úÖ **Unanimous Verdict:** All models agree on the direction. Robust Signal.")
            else:
                st.warning("‚ö†Ô∏è **Mixed Verdict:** Models disagree. Weak Signal.")
            st.info(f"**X-Learner Estimate (Winner):** {x_mean:.3f}")

    # ==========================
    # TAB 4: Evaluation (Unified Benchmark - 3 FOLD DML vs 3 FOLD OLS)
    # ==========================
    with tab4:
        st.subheader("‚öñÔ∏è Methodology Evaluation")
        
        st.markdown("""
        <div class='theory-card'>
        <b>Why DML? (3-Fold Cross-Fitting)</b><br>
        Standard models confuse correlation with causation. To fix this, we use the <b>Frisch-Waugh-Lovell (FWL)</b> theorem.<br>
        1. We split data into 10 folds.<br>
        2. We train models to predict T and Y using ONLY confounders.<br>
        3. We subtract these predictions to get "Residuals" (The part of T and Y that History <i>cannot</i> explain).<br>
        4. The correlation between these Residuals is the <b>True Causal Effect</b>.
        </div>
        """, unsafe_allow_html=True)
        
        if st.session_state.get('fold_metrics') is None or st.session_state.get('ols_fold_metrics') is None:
            kf = KFold(n_splits=10, shuffle=True, random_state=42)
            fold_metrics = []
            ols_fold_metrics = []
            
            with st.spinner("Running 3Fold Stability Check (DML vs OLS)..."):
                for fold_idx, (train_idx, val_idx) in enumerate(kf.split(train_df)):
                    # Data Split
                    X_train_f, X_val_f = train_df.iloc[train_idx], train_df.iloc[val_idx]
                    
                    # 1. DML Estimate (On Validation Fold)
                    fold_engine = RealCausalEngine()
                    fold_engine.train(X_train_f, target_col, treatment_col, all_confounders, heterogeneity_cols=['Latent_Market_State'])
                    fold_effects = fold_engine.get_causal_effect(X_val_f[['Latent_Market_State']])
                    fold_metrics.append(np.mean(fold_effects))
                    
                    # 2. OLS Estimate (On Training Fold - Standard CV Logic)
                    # We fit OLS on K-1 folds and record the coefficient. This shows how much the OLS param jumps around.
                    ols = LinearRegression()
                    ols.fit(X_train_f[[treatment_col] + all_confounders], X_train_f[target_col])
                    ols_fold_metrics.append(ols.coef_[0])
            
            st.session_state['fold_metrics'] = fold_metrics
            st.session_state['ols_fold_metrics'] = ols_fold_metrics
        
        # Retrieve
        fold_metrics = st.session_state['fold_metrics']
        ols_fold_metrics = st.session_state['ols_fold_metrics']
        
        dml_avg = np.mean(fold_metrics)
        ols_avg = np.mean(ols_fold_metrics)
        
        # UNIFIED VISUALIZATION
        st.subheader("üèÜ Unified Benchmark: Stability & Performance")
        st.caption("Head-to-Head: DML Folds (Blue) vs OLS Folds (Red). Are they fighting?")

        std_folds = np.std(fold_metrics)
        std_ols = np.std(ols_fold_metrics)
        
        col_m1, col_m2, col_m3 = st.columns(3)
        col_m1.metric("DML Stability (Std Dev)", f"{std_folds:.3f}", 
                      delta="Stable" if std_folds < 0.2 else "Volatile",
                      help="DML Variation across 10 folds.")
        col_m2.metric("OLS Stability (Std Dev)", f"{std_ols:.3f}",
                      help="OLS Variation across 10 folds (Usually low, but biased).")
        col_m3.metric("Bias Gap (Average)", f"{dml_avg - ols_avg:.3f}", 
                      delta_color="inverse",
                      help="Difference between DML and OLS averages.")

        # Grouped Bar Chart
        fig_unified = go.Figure()
        
        # DML Bars
        fig_unified.add_trace(go.Bar(
            x=[f"Fold {i+1}" for i in range(10)],
            y=fold_metrics,
            name='DML (Causal)',
            marker_color='#0ea5e9'
        ))
        
        # OLS Bars
        fig_unified.add_trace(go.Bar(
            x=[f"Fold {i+1}" for i in range(10)],
            y=ols_fold_metrics,
            name='OLS (Traditional)',
            marker_color='#ef4444'
        ))
        
        # Averages lines
        fig_unified.add_hline(y=dml_avg, line_dash="dash", line_color="#0ea5e9", annotation_text="DML Avg")
        fig_unified.add_hline(y=ols_avg, line_dash="dash", line_color="#ef4444", annotation_text="OLS Avg")

        fig_unified.update_layout(title="3-Fold Cross-Validation Battle", 
                                  barmode='group', 
                                  template="plotly_white", 
                                  height=500)
        st.plotly_chart(fig_unified, use_container_width=True)
        
        # Scorecard
        st.markdown("### üö¶ Causal Scorecard")
        sc1, sc2, sc3 = st.columns(3)
        
        if std_folds < 0.1:
            sc1.success(f"‚úÖ **Excellent Stability**\n\nDML Std: {std_folds:.3f}")
        elif std_folds < 0.2:
            sc1.warning(f"‚ö†Ô∏è **Acceptable Stability**\n\nDML Std: {std_folds:.3f}")
        else:
            sc1.error(f"üõë **Unstable**\n\nDML Std: {std_folds:.3f}")
            
        bias_gap = abs(dml_avg - ols_avg)
        if bias_gap > 0.1:
            sc2.success(f"‚úÖ **High Value Discovery**\n\nGap: {bias_gap:.3f}")
        elif bias_gap > 0.05:
            sc2.warning(f"‚ö†Ô∏è **Moderate Correction**\n\nGap: {bias_gap:.3f}")
        else:
            sc2.info(f"‚ÑπÔ∏è **Low Bias**\n\nGap: {bias_gap:.3f}")
            
        if dml_avg < 0:
            sc3.success(f"‚úÖ **Logical Direction**\n\nNegative Elasticity")
        else:
            sc3.error(f"üõë **Anomalous**\n\nPositive Elasticity")

    # ==========================
    # TAB 5: Report (INTEGRATED & FIXED)
    # ==========================
    with tab5:
        st.subheader("üìã Executive Summary")
        
        if st.session_state.get('cate_results') is None:
             with st.spinner("Generating Tournament Results for Report..."):
                 meta_feats = all_confounders + ['Latent_Market_State']
                 st.session_state['cate_results'] = train_meta_learners(df_eng, target_col, treatment_col, meta_feats)
        
        if st.session_state.get('fold_metrics') is None:
             # Just trigger the calc from Tab 4 logic if missing
             pass 

        cate_res = st.session_state['cate_results']
        winner_avg = cate_res['X-Learner'].mean()
        f_mets = st.session_state.get('fold_metrics', [0])
        ols_mets = st.session_state.get('ols_fold_metrics', [0])
        
        dml_avg_r = np.mean(f_mets)
        ols_avg_r = np.mean(ols_mets)
        std_dev = np.std(f_mets)
        gap = dml_avg_r - ols_avg_r

        report_txt = f"""
        CAUSAL AI EXECUTIVE SUMMARY
        ===========================
        
        1. EXECUTIVE FINDINGS (Double Machine Learning)
        -----------------------------------------------
        - True Causal Elasticity: {avg_elasticity:.4f}
        - Naive Correlation: {naive_corr:.4f}
        - Bias Detected: {bias_delta:.4f}
        
        2. METHODOLOGY AUDIT
        --------------------
        - Algorithm: Double Machine Learning (LinearDML) with 3-Fold Cross-Fitting
        - DML Stability (Std Dev): {std_dev:.4f} ({'Stable' if std_dev < 0.2 else 'Volatile'})
        - OLS Stability (Std Dev): {np.std(ols_mets):.4f}
        - Superiority vs OLS: The DML model corrected a bias of {gap:.4f} vs Traditional Regression.
        
        3. MODEL TOURNAMENT RESULTS
        ---------------------------
        - Winning Architecture: X-Learner (Gradient Boosting)
        - Estimated Uplift: {winner_avg:.4f}
        - Consensus: S-Learner and T-Learner confirmed directionality.
        
        4. OPERATIONAL FORECAST
        -----------------------
        Scenario: {treatment_col} adjusted to {price_main:.2f}
        - Projected Total Outcome: {total_sim:,.0f}
        - Recommended Safety Stock: {opt_stock_main:,.0f} (95% Service Level)
        
        5. STRATEGIC RECOMMENDATION
        ---------------------------
        Use the Heterogeneity Chart (Tab 1) to identify "High Momentum" periods.
        Raise prices ONLY during those periods to minimize volume loss.
        """
        
        st.text_area("Report Preview", report_txt, height=400)
        
        if st.button("üìÑ Download PDF Report"):
            pdf = FPDF()
            pdf.add_page()
            pdf.set_font("Arial", 'B', 16)
            pdf.cell(0, 10, "Causal AI Analysis Report", 0, 1, 'C')
            pdf.set_font("Arial", size=11)
            pdf.ln(10)
            pdf.multi_cell(0, 7, report_txt)
            pdf_out = pdf.output(dest='S').encode('latin-1')
            st.download_button("Download PDF", pdf_out, "causal_report.pdf", "application/pdf")